<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Nikhil Kumar Thota</title>
<meta name="generator" content="Jekyll v4.3.3" />
<meta property="og:title" content="Nikhil Kumar Thota" />
<meta name="author" content="T-NIKHIL" />
<meta property="og:locale" content="en_US" />
<link rel="canonical" href="http://localhost:4000/regularization/" />
<meta property="og:url" content="http://localhost:4000/regularization/" />
<meta property="og:site_name" content="Nikhil Kumar Thota" />
<meta property="og:type" content="website" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Nikhil Kumar Thota" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"WebPage","author":{"@type":"Person","name":"T-NIKHIL"},"headline":"Nikhil Kumar Thota","url":"http://localhost:4000/regularization/"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="http://localhost:4000/feed.xml" title="Nikhil Kumar Thota" />

<script>
MathJax = {
  tex : {
    inlineMath: [['$', '$'], ['\\(', '\\)']]
  },
  svg: {
    fontCache: 'global'
  }
}; 
</script>

<script type="text/javascript" 
id="MathJax-script" 
async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js"></script></head>
<body><header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" href="/">Nikhil Kumar Thota</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/cv/">CV</a><a class="page-link" href="/research/">Research</a><a class="page-link" href="/blogs/">Blogs</a><a class="page-link" href="/sports/">Sports</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post">

  <!-- <header class="post-header">
    <h1 class="post-title"></h1>
  </header> -->

  <div class="post-content">
    <p align="justify">
    As scientists and engineers when we build a mathemetical model 
    to describe a process or system, we often start with the simplest
    explanation or model. This line of thinking is pervasive throughout
    science and engineering and goes by many names (Ex : Occam's razor).
    Simple models are not only more intuitive and interpretable, but are also
    more likely to generalize outside the domain it was built on. This guiding principle has found its way into the world of machine learning and led to devleopment of several 
    strategies to train machine learning models to generalize better. These strategies collectively fall under the name of <em>regularization</em>. In this tutorial we will be looking at
    parameter normed penalties. 
</p>

<p align="center">
    <img src="/assets/occams_razor.pdf" alt="Occam's Razor" />
</p>

<p align="justify">
    Lets start by building a toy dataset with 2 variables
    ($x_1$ and $x_2$) and a dependent variable $y$ that 
    linearly depends on the two independent variables 
    as $y = 3x_1 + 2x_2 + N(0,0.5)$. Code to generate
    the dataset is shown below. 
</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">seed</span> <span class="o">=</span> <span class="mi">777</span>
<span class="n">num_data</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">w</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">]]).</span><span class="nf">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">f</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
    <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">noise</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
    <span class="n">random</span><span class="p">.</span><span class="nf">noise</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
    <span class="n">gaussian_noise</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]).</span><span class="nf">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">np</span><span class="p">.</span><span class="nf">dot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">w</span><span class="p">)</span> <span class="o">+</span> <span class="mf">0.5</span><span class="o">*</span><span class="n">gaussian_noise</span>

<span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
<span class="n">random</span><span class="p">.</span><span class="nf">seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
<span class="n">x1</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">round</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">uniform</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">num_data</span><span class="p">),</span> <span class="mi">1</span><span class="p">).</span><span class="nf">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">x2</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">round</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">uniform</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">num_data</span><span class="p">),</span> <span class="mi">1</span><span class="p">).</span><span class="nf">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

<span class="n">features</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">hstack</span><span class="p">((</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">))</span>
<span class="n">target</span> <span class="o">=</span> <span class="nf">f</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">features</span><span class="p">)</span>
<span class="n">dataset</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">hstack</span><span class="p">((</span><span class="n">features</span><span class="p">,</span> <span class="n">target</span><span class="p">))</span>
</code></pre></div></div>

<p align="justify">
    We can visualize the surface without the gaussian noise and
    is shown in red in the below figure. The black dots are the
    dataset we will use to train our model.
</p>

<iframe id="igraph" scrolling="yes" style="border:none; margin-bottom:0" seamless="seamless" src="/assets/blogs/regularization/linear_surface.html" height="380" width="740"></iframe>

<p>Our linear model is $\hat{y} = w_1x_1 + w_2x_2$. 
Letâ€™s also define a loss function that will tell us how good our model
parameters $w_1$ and $w_2$ are. For this we will use
the mean squared error (MSE) loss function.</p>

\[J = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2\]

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">LinearModel</span><span class="p">():</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="n">self</span><span class="p">.</span><span class="n">w</span> <span class="o">=</span> <span class="bp">None</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">save</span><span class="o">=</span><span class="bp">True</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">save</span><span class="p">:</span>
            <span class="n">self</span><span class="p">.</span><span class="n">w</span> <span class="o">=</span> <span class="n">w</span>
        <span class="k">return</span> <span class="n">np</span><span class="p">.</span><span class="nf">dot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">w</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">loss_fn</span><span class="p">(</span><span class="n">targets</span><span class="p">,</span> <span class="n">preds</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">np</span><span class="p">.</span><span class="nf">mean</span><span class="p">((</span><span class="n">targets</span><span class="p">.</span><span class="nf">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> <span class="o">-</span> <span class="n">preds</span><span class="p">.</span><span class="nf">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
</code></pre></div></div>

<p>Lets explore how this loss function looks like in 3D.
The dotted black line shows where the global minima exists for this surface.</p>

<iframe id="igraph" scrolling="yes" style="border:none; margin-bottom:0" seamless="seamless" src="/assets/blogs/regularization/loss_surface.html" height="380" width="740"></iframe>

<p>Now lets write a simple code to do gradient descent to update
our model parameters.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">dl_dw1</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="n">preds</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">np</span><span class="p">.</span><span class="nf">mean</span><span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="p">(</span><span class="n">dataset</span><span class="p">[:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">].</span><span class="nf">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> <span class="o">-</span> <span class="n">preds</span><span class="p">.</span><span class="nf">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span><span class="o">*</span><span class="p">(</span><span class="o">-</span><span class="n">dataset</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">].</span><span class="nf">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)))</span>

<span class="k">def</span> <span class="nf">dl_dw2</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="n">preds</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">np</span><span class="p">.</span><span class="nf">mean</span><span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="p">(</span><span class="n">dataset</span><span class="p">[:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">].</span><span class="nf">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> <span class="o">-</span> <span class="n">preds</span><span class="p">.</span><span class="nf">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span><span class="o">*</span><span class="p">(</span><span class="o">-</span><span class="n">dataset</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">].</span><span class="nf">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)))</span>

<span class="c1"># Initial value to start optimization from
</span><span class="n">w_init</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([[</span><span class="o">-</span><span class="mi">10</span><span class="p">,</span> <span class="o">-</span><span class="mi">10</span><span class="p">]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">float32</span><span class="p">).</span><span class="nf">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">num_epochs</span> <span class="o">=</span> <span class="mi">40</span>
<span class="c1"># Learning rate
</span><span class="n">lr</span> <span class="o">=</span> <span class="mf">0.1</span>

<span class="n">linear_model</span> <span class="o">=</span> <span class="nc">LinearModel</span><span class="p">()</span>
<span class="n">preds</span> <span class="o">=</span> <span class="n">linear_model</span><span class="p">.</span><span class="nf">forward</span><span class="p">(</span><span class="n">dataset</span><span class="p">[:,:</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">w_init</span><span class="p">)</span>
<span class="n">preds_hist</span> <span class="o">=</span> <span class="n">preds</span><span class="p">.</span><span class="nf">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">w_hist</span> <span class="o">=</span> <span class="nf">deepcopy</span><span class="p">(</span><span class="n">w_init</span><span class="p">)</span>
<span class="n">w_new</span> <span class="o">=</span> <span class="nf">deepcopy</span><span class="p">(</span><span class="n">w_init</span><span class="p">)</span>

<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">num_epochs</span><span class="p">):</span>
    <span class="n">w_grads</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span><span class="nf">dl_dw1</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="n">preds</span><span class="p">),</span> <span class="nf">dl_dw2</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="n">preds</span><span class="p">)]).</span><span class="nf">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">w_new</span> <span class="o">-=</span> <span class="n">lr</span><span class="o">*</span><span class="n">w_grads</span>
    <span class="n">preds</span> <span class="o">=</span> <span class="n">linear_model</span><span class="p">.</span><span class="nf">forward</span><span class="p">(</span><span class="n">train_dataset</span><span class="p">[:,:</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">w_new</span><span class="p">)</span>
    <span class="n">w_hist</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">hstack</span><span class="p">((</span><span class="n">w_hist</span><span class="p">,</span> <span class="n">w_new</span><span class="p">))</span>
    <span class="n">preds_hist</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">hstack</span><span class="p">((</span><span class="n">preds_hist</span><span class="p">,</span> <span class="n">preds</span><span class="p">.</span><span class="nf">rehape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)))</span>
</code></pre></div></div>

<iframe id="igraph" scrolling="yes" style="border:none; 
margin-bottom:0" seamless="seamless" src="/assets/blogs/regularization/GD_path_no_l1_reg.html" height="380" width="740"></iframe>

<p align="justify">
    The left plot in the above figure traces out the parameter trajectory 
    in the parameter space and the right plot shows the fitting 
    with the final model parameters. The algorithm is able to 
    find parameters close to the true parameters. 
</p>

<p align="justify">
    Now consider the case where instead of just fitting 2 variables,
    we have to fit a linear model with many more variables than data
    available to train the model. For this lets double the number of variables while still maintaining the number of datapoints.
</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">num_data</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">w_true</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]]).</span><span class="nf">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

<span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
<span class="n">random</span><span class="p">.</span><span class="nf">seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
<span class="n">x1</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">round</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">uniform</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">num_data</span><span class="p">),</span> <span class="mi">1</span><span class="p">).</span><span class="nf">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">x2</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">round</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">uniform</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">num_data</span><span class="p">),</span> <span class="mi">1</span><span class="p">).</span><span class="nf">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">x3</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">round</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">uniform</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">num_data</span><span class="p">),</span> <span class="mi">1</span><span class="p">).</span><span class="nf">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">x4</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">round</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">uniform</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">num_data</span><span class="p">),</span> <span class="mi">1</span><span class="p">).</span><span class="nf">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

<span class="n">features</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">hstack</span><span class="p">((</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">))</span>
<span class="n">target</span> <span class="o">=</span> <span class="nf">f</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">features</span><span class="p">)</span>
<span class="n">dataset</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">hstack</span><span class="p">((</span><span class="n">features</span><span class="p">,</span> <span class="n">target</span><span class="p">))</span>

<span class="n">w_init</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([[</span><span class="o">-</span><span class="mi">10</span><span class="p">,</span> <span class="o">-</span><span class="mi">10</span><span class="p">,</span> <span class="o">-</span><span class="mi">10</span><span class="p">,</span> <span class="o">-</span><span class="mi">10</span><span class="p">]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">float32</span><span class="p">).</span><span class="nf">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">num_epochs</span> <span class="o">=</span> <span class="mi">600</span>
<span class="n">lr</span> <span class="o">=</span> <span class="mf">0.1</span>

<span class="n">linear_model</span> <span class="o">=</span> <span class="nc">LinearModel</span><span class="p">()</span>
<span class="n">preds</span> <span class="o">=</span> <span class="n">linear_model</span><span class="p">.</span><span class="nf">forward</span><span class="p">(</span><span class="n">dataset</span><span class="p">[:,:</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">w_init</span><span class="p">)</span>
<span class="n">preds_hist</span> <span class="o">=</span> <span class="n">preds</span><span class="p">.</span><span class="nf">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">loss</span> <span class="o">=</span> <span class="nf">loss_fn</span><span class="p">(</span><span class="n">dataset</span><span class="p">[:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">preds</span><span class="p">)</span>
<span class="n">loss_hist</span> <span class="o">=</span> <span class="n">loss</span><span class="p">.</span><span class="nf">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">w_hist</span> <span class="o">=</span> <span class="nf">deepcopy</span><span class="p">(</span><span class="n">w_init</span><span class="p">)</span>
<span class="n">w_new</span> <span class="o">=</span> <span class="nf">deepcopy</span><span class="p">(</span><span class="n">w_init</span><span class="p">)</span>

<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">num_epochs</span><span class="p">):</span>
    <span class="n">w_grads</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span><span class="nf">dl_dw1</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="n">preds</span><span class="p">,</span> <span class="n">w_new</span><span class="p">,</span> <span class="n">lambda_reg</span><span class="p">),</span> 
                        <span class="nf">dl_dw2</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="n">preds</span><span class="p">,</span> <span class="n">w_new</span><span class="p">,</span> <span class="n">lambda_reg</span><span class="p">),</span>
                        <span class="nf">dl_dw3</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="n">preds</span><span class="p">,</span> <span class="n">w_new</span><span class="p">,</span> <span class="n">lambda_reg</span><span class="p">),</span>
                        <span class="nf">dl_dw4</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="n">preds</span><span class="p">,</span> <span class="n">w_new</span><span class="p">,</span> <span class="n">lambda_reg</span><span class="p">)]).</span><span class="nf">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">w_new</span> <span class="o">-=</span> <span class="n">lr</span> <span class="o">*</span> <span class="n">w_grads</span>
    <span class="n">preds</span> <span class="o">=</span> <span class="n">linear_model</span><span class="p">.</span><span class="nf">forward</span><span class="p">(</span><span class="n">dataset</span><span class="p">[:,:</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">w_new</span><span class="p">)</span>
    <span class="n">preds_hist</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">hstack</span><span class="p">((</span><span class="n">preds_hist</span><span class="p">,</span> <span class="n">preds</span><span class="p">.</span><span class="nf">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)))</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="nf">loss_fn</span><span class="p">(</span><span class="n">dataset</span><span class="p">[:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">preds</span><span class="p">)</span>
    <span class="n">loss_hist</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">hstack</span><span class="p">((</span><span class="n">loss_hist</span><span class="p">,</span> <span class="n">loss</span><span class="p">.</span><span class="nf">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)))</span>
    <span class="n">w_hist</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">hstack</span><span class="p">((</span><span class="n">w_hist</span><span class="p">,</span> <span class="n">w_new</span><span class="p">))</span>
</code></pre></div></div>

<p align="center">
    <img src="/assets/blogs/regularization/loss_curve_for_4_feats.pdf" alt="Loss Curve for 4 features" />
</p>

<p align="justify">
    One thing we immediately observe is that the number of epochs has
    increased by almost 20x by just adding 2 more independent variables to
    the dataset. In the loss plot we see a steep drop in the first 50 epochs and 
    then a gradual decline over the next 300 epochs. We
    see by 300 epochs, the model has found that the first 2 variables
    are the major contributors to the target variable prediction. Over the 
    next 300 epochs, the loss curve starts to plateau and the model
    refines its parameter estimates.
</p>

<p align="justify">
    <b>Exercise 1</b> : Tune the learning rate, number of epochs and
    initial parameter guesses to see if you can bring the loss down
    further. Are you able to find better parameter estimates ?
</p>

<p align="justify">
    After playing around for a while, you will realize that the model
    gets close to the true parameter set but never actually reaches it.
    Why does this happen ? This is because the model we have is too complex
    for the dataset we are trying to fit. The model tries to use all available 
    variables given to it but doesn't realize that only the first 2 variables
    are sufficient to explain the target variable. After thinking for a while
    you might come up with a solution to add a constraint on the model
    parameters to be as small as possible. How can we mathemetically formalize
    this constraint ? This is exactly with parameter norm penalties do. 
</p>

<p align="justify">
    Wolfram Mathworld defines a norm as a quantity that in an abstract
    sense defines length, size or extent of an object. We can think of our
    model parameters as a rank-1 tensor (i.e vector). For any real number
    p &gt;= 1, the p-norm of a vector is defined as below, where 'n' is the
    number of model parameters and 'p' defines the type of norm we are using.
</p>

\[||w||_p = (|x_1|^p + |x_2|^p + |x_3|^p \dots + |x_n|^p)^{\frac{1}{p}}\]

<p align="justify">
    Substituting p=1, we get the 1-norm also called $L^1$ norm or Manhattan norm.
    Substituting p=2, we get the famous 2-norm also called $L^2$ norm or Euclidean norm. The $L^\infty$ norm or maximum norm is the limit of the $L^p$
    norm as $p \to \infty$. The $L^\infty$ norm is interesting as only points at the corner of the square satisfy the constraint. The below plot shows the level sets of $L^1$, $L^2$, $L^3$ and $L^\infty$ norms in 2D.
</p>

<p align="center">
    <img src="/assets/blogs/regularization/norms.pdf" alt="L1 to Linf norm" />
</p>

<p align="justify">
    Lets start by adding the $L^1$ norm constraint to our loss function and stick with the 2 parameter case so we can visualize
    the loss surface. The loss function we arrived at is the same loss
    function used in least absolute shrinkage and selection operator (LASSO).
</p>

\[J = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2 + \lambda(|w_{1}| + |w_{2}|)\]

<p align="justify">
    The $\lambda$ parameter controls the strength of the
    $L^1$ norm penalty. For now lets fix the value to 0.5.
</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Updating loss and grad functions to include L1 norm penalty ...
</span><span class="k">def</span> <span class="nf">loss_fn</span><span class="p">(</span><span class="n">targets</span><span class="p">,</span> <span class="n">preds</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">lambda_reg</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">np</span><span class="p">.</span><span class="nf">mean</span><span class="p">((</span><span class="n">targets</span><span class="p">.</span><span class="nf">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> <span class="o">-</span> <span class="n">preds</span><span class="p">.</span><span class="nf">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span> <span class="o">+</span> <span class="n">lambda_reg</span><span class="o">*</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">abs</span><span class="p">(</span><span class="n">w</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">])</span> <span class="o">+</span> <span class="n">np</span><span class="p">.</span><span class="nf">abs</span><span class="p">(</span><span class="n">w</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">]))</span>

<span class="k">def</span> <span class="nf">dl_dw1</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="n">preds</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">lambda_reg</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">np</span><span class="p">.</span><span class="nf">mean</span><span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="p">(</span><span class="n">dataset</span><span class="p">[:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">].</span><span class="nf">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> <span class="o">-</span> <span class="n">preds</span><span class="p">.</span><span class="nf">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span><span class="o">*</span><span class="p">(</span><span class="o">-</span><span class="n">dataset</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">].</span><span class="nf">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)))</span> <span class="o">+</span> <span class="n">lambda_reg</span><span class="o">*</span><span class="n">np</span><span class="p">.</span><span class="nf">sign</span><span class="p">(</span><span class="n">w</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">])</span>

<span class="k">def</span> <span class="nf">dl_dw2</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="n">preds</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">lambda_reg</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">np</span><span class="p">.</span><span class="nf">mean</span><span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="p">(</span><span class="n">dataset</span><span class="p">[:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">].</span><span class="nf">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> <span class="o">-</span> <span class="n">preds</span><span class="p">.</span><span class="nf">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span><span class="o">*</span><span class="p">(</span><span class="o">-</span><span class="n">dataset</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">].</span><span class="nf">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)))</span> <span class="o">+</span> <span class="n">lambda_reg</span><span class="o">*</span><span class="n">np</span><span class="p">.</span><span class="nf">sign</span><span class="p">(</span><span class="n">w</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">])</span>
</code></pre></div></div>

<iframe id="igraph" scrolling="yes" style="border:none; 
margin-bottom:0" seamless="seamless" src="/assets/blogs/regularization/loss_surface_with_l1_reg.html" height="380" width="740"></iframe>

<p align="justify">
    Where the MSE loss surface and $L^1$ norm constraint meet, defines the set of 
    solutions that satisfy the constraint. Now the optimizer must find where along
    this 1-D curve the loss is minimized. Sticking with the same num_epochs and 
    learning rate as in the case without $L^1$ norm constraint lets run our gradient descent code and visualize the parameter 
    trajectory and final optimized model parameters.
</p>

<iframe id="igraph" scrolling="yes" style="border:none; 
margin-bottom:0" seamless="seamless" src="/assets/blogs/regularization/GD_path_with_l1_reg.html" height="380" width="740"></iframe>

<p align="justify">
    Not suprisingly, the model performs even worse. The surface 
    defined by the $L^1$ norm penalty actually pushes the model
    parameters back and prevents it from reaching the global minimum.
</p>

<p align="justify">
    <b>Exercise 3</b> : Vary the $\lambda$ parameter and see how it affects
    the loss surface and the parameter trajectory on the loss surface.
</p>

<p align="justify">
    <b>Exercise 4</b> : Instead of the $L^{1}$ norm parameter penalty, change
    the loss function and grad functions to include the $L^{2}$ norm
    penalty. The loss function you arrive at is the one used in Ridge
    Regression. Visualize how this loss surface looks like. Run gradient descent and see what model parameters you arrive at.
</p>

<p align="justify">
    Now that we have some intuition of what is happening, lets go 
    back to fitting the model to the 4D dataset. 
</p>

<p align="center">
    <img src="/assets/blogs/regularization/loss_curve_for_4_feats_w_l1_reg.pdf" alt="loss curve with 4 feats" />
</p>

<p align="justify">
    We see that the loss curve plateaus very early
    on with high regularization and the parameters
    weighting the last 2 features are the smallest. $\lambda=0.1$ offers a good balance
    between accuracy and minimizing influence
    of features 3 and 4. $\lambda$ is called as
    a hyperparameter as it is not a parameter of the model
    but has an impact during the model training. The
    sequentially thresholded least squares (STLSQ) algorithm
    modifies the LASSO algorithm to suppress model parameters
    below a set threshold. The parameter vector obtained at the
    end of training is a sparse vector. This optimization
    algorithm is used in sparse identification of nonlinear dynamics
    (SINDy) [1].
</p>

<p align="justify">
[1] S.L. Brunton, J.L. Proctor, &amp; J.N. Kutz, Discovering governing equations from data by sparse identification of nonlinear dynamical systems, Proc. Natl. Acad. Sci. U.S.A. 113 (15) 3932-3937, https://doi.org/10.1073/pnas.1517384113 (2016).
</p>

  </div>

  

<script>
MathJax = {
  tex : {
    inlineMath: [['$', '$'], ['\\(', '\\)']]
  },
  svg: {
    fontCache: 'global'
  }
}; 
</script>

<script type="text/javascript" 
id="MathJax-script" 
async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js"></script>

</article>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <!-- <p class="feed-subscribe">
          <a href="http://localhost:4000/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p> -->
        <div class="social-links"><ul class="social-media-list"><li>
  <a rel="me" href="https://github.com/T-NIKHIL" target="_blank" title="github">
    <svg class="svg-icon grey">
      <use xlink:href="/assets/minima-social-icons.svg#github"></use>
    </svg>
  </a>
</li>
<li>
  <a rel="me" href="https://scholar.google.com/citations?user=2ewiheKHM4gC&hl=en" target="_blank" title="google_scholar">
    <svg class="svg-icon grey">
      <use xlink:href="/assets/minima-social-icons.svg#google_scholar"></use>
    </svg>
  </a>
</li>
<li>
  <a rel="me" href="https://www.linkedin.com/in/nikhil-thota/" target="_blank" title="linkedin">
    <svg class="svg-icon grey">
      <use xlink:href="/assets/minima-social-icons.svg#linkedin"></use>
    </svg>
  </a>
</li>
<li>
  <a rel="me" href="https://x.com/nik_thota" target="_blank" title="x">
    <svg class="svg-icon grey">
      <use xlink:href="/assets/minima-social-icons.svg#x"></use>
    </svg>
  </a>
</li>
</ul>
</div>
          <ul class="contact-list">
            <p align="center">
              Email : <a class="u-email" href="mailto:nthota2@jhu.edu">nthota2@jhu.edu</a>
            </p>
          </ul>
          <p align="center"> Inspired from <a href="https://github.com/jekyll/minima">Jekyll Minima</a>.
        </p>
      </div>
    </div>

  </div>

</footer>
</body>

</html>
