<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2024-08-27T16:37:05-04:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Nikhil Kumar Thota</title><author><name>T-NIKHIL</name><email>nthota2@jhu.edu</email></author><entry><title type="html">Pytorch Vs Tensorflow</title><link href="http://localhost:4000/2024/08/26/Pytorch-vs-TensorFlow.html" rel="alternate" type="text/html" title="Pytorch Vs Tensorflow" /><published>2024-08-26T00:00:00-04:00</published><updated>2024-08-26T00:00:00-04:00</updated><id>http://localhost:4000/2024/08/26/Pytorch-vs-TensorFlow</id><content type="html" xml:base="http://localhost:4000/2024/08/26/Pytorch-vs-TensorFlow.html"><![CDATA[<!--more-->
<p>This is included in excerpts.</p>

<p>This is also included in excerpts.</p>

<!--more-->

<ul>
  <li>PyTorch and TensorFlow are open source machine learning libraries in Python.</li>
  <li>PyTorch : <a href="https://arxiv.org/abs/1912.01703">Technical Paper</a> , <a href="https://github.com/pytorch">Github Repo</a></li>
  <li>TensorFlow : <a href="https://arxiv.org/abs/1605.08695">Technical Paper</a>, <a href="https://github.com/tensorflow">Github Repo</a></li>
  <li>Underlying code of PyTorch uses libtorch which is the machine learning library in C++. All the tensor creation, GPU, CPU operations and parallel primitives have an underlying C++ code. One advanatge of doing this is it allows to get around the Global Interpreter Lock (GIL) in Python and allow multi threading execution. Tensorflow also has a C based code.</li>
  <li>Developer Teams : PyTorch was developed mostly by engineers at Meta while TensorFlow was created by engineers at Google Brain.</li>
  <li>Timeline : PyTorch was released in October of 2016 while TensorFlow was released in November of 2015.</li>
  <li>Training visualization : TensorFlow has TensorBoard which makes it easier to log errors, view the computation graph and  weights. PyTorch has no default viewing option and uses TensorBoard.</li>
</ul>

<h1 id="creating-tensors">Creating Tensors</h1>

<p>A tensors is a multidimensional arrays similar to numpy.
Unlike numpy arrays tensors are immutable. 
Which basically means that changing the contents 
of the original array results in a copy of the array.
Tensors are also optimized to run on specialized hardware
like Graphical Processing Unit (GPU) and Tensor Processing Unit (TPU).</p>

<h2 id="pytorch">PyTorch</h2>

<p>In PyTorch some of the ways a tensor can be created are as follows:</p>
<ul>
  <li>torch.tensor() : Create from list, tuple, nd.array, scaler etc ..</li>
  <li>torch.from_numpy() : Create specifically from numpy array. Resulting tensor and numpy array share the same memory.</li>
  <li>torch.ones(), torch.ones_like() : Create a ones tensor with same shape and dtype as the provided input.</li>
  <li>torch.zeros(), torch.zeros_like() : Create a zeros tensor with same shape and dtype as the provided input.</li>
  <li>torch.empty()</li>
  <li>torch.complex()</li>
</ul>

<p>Check out creation ops on PyTorch website for more ways to create a tensor.
When instantiating the tensor using torch.tensor, the user can specify the dtype, 
device and require_grad (this should be specified if the tensor is part of a computation graph).</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">nested_list</span> <span class="o">=</span> <span class="p">[[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span>
               <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">]]</span>
<span class="c1"># Creating a 2D tensor with PyTorch
</span><span class="n">torch_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">(</span><span class="n">nested_list</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="sh">'</span><span class="s">cpu</span><span class="sh">'</span><span class="p">)</span></code></pre></figure>

<p>If the device option is not specified then
by default the tensor is instatiated on the CPU.
You can move it later to a CUDA enabled GPU by
using the torch.tensor.to() function.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="k">if</span> <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="nf">is_available</span><span class="p">():</span>
  <span class="n">torch_tensor</span> <span class="o">=</span> <span class="n">torch_tensor</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="sh">'</span><span class="s">cuda</span><span class="sh">'</span><span class="p">)</span></code></pre></figure>

<h2 id="tensorflow">TensorFlow</h2>

<p>In TensorFlow some of the ways a tensor can be created are as follows:</p>

<ul>
  <li>tf.constant(): Can create a tensor from a constant value or list.</li>
  <li>tf.convert_to_tensor(): Create from list, scaler and numpy arrays.</li>
  <li>tf.ones(), tf.ones_like()</li>
  <li>tf.zeros(), tf.zeros_like()</li>
  <li>tf.experimental.numpy.empty()</li>
  <li>tf.complex()</li>
</ul>

<p>When instantiating the tensor, the user specifies the data and the dtype.
To manually place the tensor on the device you want to do 
computation on use the context manager.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Num GPUs available : </span><span class="si">{</span><span class="n">tf</span><span class="p">.</span><span class="n">config</span><span class="p">.</span><span class="nf">list_physical_devices</span><span class="p">(</span><span class="sh">'</span><span class="s">GPU</span><span class="sh">'</span><span class="p">)</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
<span class="c1"># Creating a 2D tensor with TensorFlow
</span><span class="n">tf_tensor</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="nf">constant</span><span class="p">(</span><span class="n">nested_list</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="p">.</span><span class="n">float32</span><span class="p">)</span>

<span class="k">with</span> <span class="n">tf</span><span class="p">.</span><span class="nf">device</span><span class="p">(</span><span class="sh">'</span><span class="s">/CPU:0</span><span class="sh">'</span><span class="p">):</span>
  <span class="n">tf_tensor_on_cpu</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="nf">constant</span><span class="p">(</span><span class="n">nested_list</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="p">.</span><span class="n">float32</span><span class="p">)</span>
  <span class="nf">print</span><span class="p">(</span><span class="n">tf_tensor_on_cpu</span><span class="p">.</span><span class="n">device</span><span class="p">)</span></code></pre></figure>

<h1 id="model-execution">Model execution</h1>

<p>There are two modes of execution in PyTorch and TensorFlow. These are graph execution and eager execution.</p>
<ul>
  <li>Graph execution :
    <ul>
      <li>In this mode operations to be perfomed are specified as a directed acyclic graph.</li>
      <li>This requires knowing prior what are all the operations to be performed. See below for an example of a graph.</li>
      <li>Results of any intermediate operations cannot be probed as the graph executes.</li>
      <li>This is in contrast to how Python works where we can access and view the output of any operation.</li>
      <li>Graph execution is useful in the case when we want are code to be platform and language agnostic. For example in edge devices, backend servers we may not have a Python interpreter.</li>
      <li>The computation graph can be split up into sub graphs to allow for parallel execution.</li>
      <li>Graph optimizations like Grappler allow you to speed up computations by
        <ul>
          <li>Simplifying arithmetic expresssion</li>
          <li>Prune nodes that have no affect on output.</li>
          <li>Memory optimizations.</li>
          <li>Much much more. Check out here : <a href="https://www.tensorflow.org/guide/graph_optimization">Grappler for Tensoflow</a>,
  <a href="https://pytorch.org/docs/stable/fx.html">FX for PyTorch</a></li>
        </ul>
      </li>
      <li>Ref : <a href="https://www.tensorflow.org/guide/intro_to_graphs">Tensorflow Intro to Graphs</a></li>
    </ul>
  </li>
  <li>Eager execution :
    <ul>
      <li>In this mode operations are executed one after another.</li>
      <li>This is how we are used to coding in Python where we ca inspect the result of operation and pass the output to subsequent operations.</li>
      <li>The benefits of this approach are :
        <ul>
          <li>Easier debugging</li>
          <li>Use all the bells and whistles Python has to offer.</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>Eager execution is the default in PyTorch 2.0 and TensorFlow 2.0 versions.</li>
</ul>

<h2 id="pytorch-1">PyTorch</h2>

<p>Lets code the above graph in PyTorch and 
perform the forward and backward pass.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="c1"># Forward Pass : Pass the inputs through all operations in the graph
</span><span class="n">X</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">([</span><span class="mf">3.0</span><span class="p">],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">generator</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nc">Generator</span><span class="p">()</span>
<span class="n">generator</span><span class="p">.</span><span class="nf">manual_seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">w</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">generator</span><span class="o">=</span><span class="n">generator</span><span class="p">)</span> <span class="c1"># Random initialization of weight
</span><span class="n">z</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">mul</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">w</span><span class="p">)</span> <span class="c1"># X1*X2 (OR) X1.mul(X2)
</span><span class="n">b</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">generator</span><span class="o">=</span><span class="n">generator</span><span class="p">)</span> <span class="c1"># Random initialization of bias
</span><span class="n">y_hat</span> <span class="o">=</span> <span class="n">z</span> <span class="o">+</span> <span class="n">b</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">([</span><span class="mf">10.0</span><span class="p">])</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="nc">MSELoss</span><span class="p">()(</span><span class="n">y_hat</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

<span class="c1"># Backward Pass : Compute the gradients of the loss wrt parameters
</span><span class="n">loss</span><span class="p">.</span><span class="nf">backward</span><span class="p">()</span>

<span class="c1"># dL/db 
</span><span class="nf">print</span><span class="p">(</span><span class="n">b</span><span class="p">.</span><span class="n">grad</span><span class="p">)</span>
<span class="c1"># dL/dw 
</span><span class="nf">print</span><span class="p">(</span><span class="n">w</span><span class="p">.</span><span class="n">grad</span><span class="p">)</span>
<span class="c1"># dL/dX
</span><span class="nf">print</span><span class="p">(</span><span class="n">X</span><span class="p">.</span><span class="n">grad</span><span class="p">)</span></code></pre></figure>

<h2 id="tensorflow-1">TensorFlow</h2>

<p>Now lets see how TensorFlow does it.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="c1"># Forward Pass : Pass the inputs through all operations in the graph
</span><span class="n">tf</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">set_seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="k">with</span> <span class="n">tf</span><span class="p">.</span><span class="nc">GradientTape</span><span class="p">(</span><span class="n">persistent</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span> <span class="k">as</span> <span class="n">tape</span><span class="p">:</span>
  <span class="n">X</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="nc">Variable</span><span class="p">([</span><span class="mf">3.0</span><span class="p">])</span>
  <span class="n">w</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="nc">Variable</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">normal</span><span class="p">([</span><span class="mi">1</span><span class="p">],</span> <span class="n">mean</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">stddev</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">0</span><span class="p">))</span>
  <span class="n">b</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="nc">Variable</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">normal</span><span class="p">([</span><span class="mi">1</span><span class="p">],</span> <span class="n">mean</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">stddev</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">0</span><span class="p">))</span>
  <span class="n">z</span> <span class="o">=</span> <span class="n">X</span><span class="o">*</span><span class="n">w</span> 
  <span class="n">y_hat</span> <span class="o">=</span> <span class="n">z</span> <span class="o">+</span> <span class="n">b</span>
  <span class="n">y</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="nc">Variable</span><span class="p">([</span><span class="mf">10.0</span><span class="p">])</span>
  <span class="n">loss</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">losses</span><span class="p">.</span><span class="nc">MeanSquaredError</span><span class="p">()(</span><span class="n">y_hat</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

<span class="c1"># dL/db
# Syntax : tape.gradient(target, source)
</span><span class="nf">print</span><span class="p">(</span><span class="n">tape</span><span class="p">.</span><span class="nf">gradient</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">b</span><span class="p">))</span>
<span class="c1"># dL/dw
</span><span class="nf">print</span><span class="p">(</span><span class="n">tape</span><span class="p">.</span><span class="nf">gradient</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">w</span><span class="p">))</span>
<span class="c1"># dL/dX
</span><span class="nf">print</span><span class="p">(</span><span class="n">tape</span><span class="p">.</span><span class="nf">gradient</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">X</span><span class="p">))</span></code></pre></figure>

<h1 id="building-a-simple-linear-regressor">Building a simple linear regressor</h1>

<p>Now lets see how we can optimize the parameters ‘w’ and ‘b’ to best fit a dataset.
In the previous section we saw a very crude way to build the linear model.
Both PyTorch and Tensorflow have module classes which we can use to build our models.
Here we will see how these model building classes and training loops vary.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="k">def</span> <span class="nf">system</span><span class="p">(</span><span class="n">X</span><span class="p">):</span>
  <span class="k">return</span> <span class="mi">3</span><span class="o">*</span><span class="n">X</span> <span class="o">+</span> <span class="mi">2</span> <span class="o">+</span> <span class="mi">2</span><span class="o">*</span><span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">X</span><span class="p">))</span>

<span class="n">num_samples</span> <span class="o">=</span> <span class="mi">50</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="n">num_samples</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="nf">system</span><span class="p">(</span><span class="n">X</span><span class="p">).</span><span class="nf">reshape</span><span class="p">(</span><span class="n">num_samples</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">X</span><span class="p">.</span><span class="nf">reshape</span><span class="p">(</span><span class="n">num_samples</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

<span class="n">plt</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span><span class="sh">'</span><span class="s">.</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">xlabel</span><span class="p">(</span><span class="sh">'</span><span class="s">X</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">ylabel</span><span class="p">(</span><span class="sh">'</span><span class="s">y</span><span class="sh">'</span><span class="p">)</span></code></pre></figure>

<p><img src="/assets/pytorch_vs_tensorflow/system.png" alt="Plot Image" style="width: 500px; height: 350px" /></p>

<h2 id="pytorch-2">PyTorch</h2>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="k">class</span> <span class="nc">LinearRegressorTorch</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">in_dim</span><span class="p">,</span> <span class="n">out_dim</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="sh">'</span><span class="s">cpu</span><span class="sh">'</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="n">float32</span><span class="p">):</span>
    <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>
    <span class="n">self</span><span class="p">.</span><span class="n">w</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="nc">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="n">in_dim</span><span class="p">,</span> <span class="n">out_dim</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">))</span>
    <span class="n">self</span><span class="p">.</span><span class="n">b</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="nc">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="n">out_dim</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">))</span>
    <span class="c1"># Register the param so that it appears in the module.parameters()
</span>    <span class="n">self</span><span class="p">.</span><span class="nf">register_parameter</span><span class="p">(</span><span class="sh">'</span><span class="s">w</span><span class="sh">'</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">w</span><span class="p">)</span>
    <span class="n">self</span><span class="p">.</span><span class="nf">register_parameter</span><span class="p">(</span><span class="sh">'</span><span class="s">b</span><span class="sh">'</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">b</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">torch</span><span class="p">.</span><span class="nf">matmul</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">w</span><span class="p">.</span><span class="n">T</span><span class="p">)</span> <span class="o">+</span> <span class="n">self</span><span class="p">.</span><span class="n">b</span>

<span class="c1"># Instantiate the model
</span><span class="n">linear_model_torch</span> <span class="o">=</span> <span class="nc">LinearRegressorTorch</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

<span class="c1"># Set the loss function
</span><span class="n">mse_loss</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="nc">MSELoss</span><span class="p">(</span><span class="n">reduction</span><span class="o">=</span><span class="sh">'</span><span class="s">mean</span><span class="sh">'</span><span class="p">)</span>

<span class="c1"># Initialize the optimizer
</span><span class="n">opt</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">optim</span><span class="p">.</span><span class="nc">SGD</span><span class="p">(</span><span class="n">linear_model_torch</span><span class="p">.</span><span class="nf">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>

<span class="c1"># Training loop
</span><span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">50</span><span class="p">):</span>
  <span class="c1"># Set model into train mode
</span>  <span class="n">linear_model_torch</span><span class="p">.</span><span class="nf">train</span><span class="p">()</span>
  <span class="n">y_hat</span> <span class="o">=</span> <span class="nf">linear_model_torch</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">from_numpy</span><span class="p">(</span><span class="n">X</span><span class="p">))</span>
  <span class="n">loss</span> <span class="o">=</span> <span class="nf">mse_loss</span><span class="p">(</span><span class="n">y_hat</span><span class="p">,</span> <span class="n">torch</span><span class="p">.</span><span class="nf">from_numpy</span><span class="p">(</span><span class="n">y</span><span class="p">))</span>
  <span class="c1"># Zero out your grads otherwise they accumulate
</span>  <span class="n">opt</span><span class="p">.</span><span class="nf">zero_grad</span><span class="p">()</span>
  <span class="n">loss</span><span class="p">.</span><span class="nf">backward</span><span class="p">()</span>
  <span class="n">opt</span><span class="p">.</span><span class="nf">step</span><span class="p">()</span>

  <span class="k">if</span> <span class="n">i</span><span class="o">%</span><span class="mi">10</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
    <span class="n">weight</span> <span class="o">=</span> <span class="n">linear_model_torch</span><span class="p">.</span><span class="nf">state_dict</span><span class="p">()[</span><span class="sh">'</span><span class="s">w</span><span class="sh">'</span><span class="p">].</span><span class="nf">item</span><span class="p">()</span>
    <span class="n">bias</span> <span class="o">=</span> <span class="n">linear_model_torch</span><span class="p">.</span><span class="nf">state_dict</span><span class="p">()[</span><span class="sh">'</span><span class="s">b</span><span class="sh">'</span><span class="p">].</span><span class="nf">item</span><span class="p">()</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">'</span><span class="s">MSE Loss </span><span class="si">{</span><span class="n">loss</span><span class="p">.</span><span class="nf">item</span><span class="p">()</span><span class="si">}</span><span class="s"> , weight </span><span class="si">{</span><span class="n">weight</span><span class="si">}</span><span class="s">, bias </span><span class="si">{</span><span class="n">bias</span><span class="si">}</span><span class="sh">'</span><span class="p">)</span>

<span class="n">linear_model_torch</span><span class="p">.</span><span class="nf">eval</span><span class="p">()</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="sh">'</span><span class="s">.</span><span class="sh">'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">'</span><span class="s">Observed Data</span><span class="sh">'</span><span class="p">)</span>
<span class="n">X_test</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">100</span><span class="p">).</span><span class="nf">reshape</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="nf">linear_model_torch</span><span class="p">(</span><span class="n">X_test</span><span class="p">).</span><span class="nf">detach</span><span class="p">().</span><span class="nf">numpy</span><span class="p">(),</span> <span class="n">label</span><span class="o">=</span><span class="sh">'</span><span class="s">Fit</span><span class="sh">'</span><span class="p">)</span></code></pre></figure>

<p><img src="/assets/pytorch_vs_tensorflow/pytorch_fit.png" alt="PyTorch Fit Image" style="width: 500px; height: 350px" /></p>

<h2 id="tensorflow-2">TensorFlow</h2>

<p>TensorFlow has two APIs for building models.</p>
<ul>
  <li>Sequential API : Useful when you have just a stack of layers. 
  PyTorch has its own Sequential implementation in torch.nn.Sequential.</li>
  <li>Functional API : Allows more user control in model building and 
  training by subclassing the respective methods from keras.models.Models.</li>
</ul>

<p>Below I build the linear regressor model using the functional API 
but the same logic can be applied to build using the sequential API.
Unlike Pytorch where layers and models both subclass torch.nn.Module, 
in Tensorflow to build custom layers you will have to subclass the 
tf.keras.layers.Layer class first and then use it in the model class. 
You cannot directly create variables in the model class as this 
will not be recoginzed as trainable params by the model.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="n">tensorflow.keras</span> <span class="kn">import</span> <span class="n">Sequential</span><span class="p">,</span> <span class="n">layers</span>
<span class="n">linear_sequential_model</span> <span class="o">=</span> <span class="nc">Sequential</span><span class="p">([</span>
    <span class="n">layers</span><span class="p">.</span><span class="nc">Dense</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="bp">None</span><span class="p">)</span>
<span class="p">])</span>

<span class="kn">import</span> <span class="n">tensorflow.keras.optimizers</span> <span class="k">as</span> <span class="n">optimizers</span>
<span class="kn">import</span> <span class="n">tensorflow.keras.losses</span> <span class="k">as</span> <span class="n">losses</span>

<span class="k">class</span> <span class="nc">LinearLayerTF</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="n">Layer</span><span class="p">):</span>
  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">in_dim</span><span class="p">,</span> <span class="n">out_dim</span><span class="p">):</span>
    <span class="nf">super</span><span class="p">(</span><span class="n">LinearLayerTF</span><span class="p">,</span> <span class="n">self</span><span class="p">).</span><span class="nf">__init__</span><span class="p">()</span>
    <span class="n">self</span><span class="p">.</span><span class="n">w</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">add_weight</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">in_dim</span><span class="p">,</span> <span class="n">out_dim</span><span class="p">),</span> <span class="n">initializer</span><span class="o">=</span><span class="sh">'</span><span class="s">random_normal</span><span class="sh">'</span><span class="p">,</span> <span class="n">trainable</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
    <span class="n">self</span><span class="p">.</span><span class="n">b</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">add_weight</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">out_dim</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">initializer</span><span class="o">=</span><span class="sh">'</span><span class="s">random_normal</span><span class="sh">'</span><span class="p">,</span> <span class="n">trainable</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">tf</span><span class="p">.</span><span class="nf">matmul</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">w</span><span class="p">)</span> <span class="o">+</span> <span class="n">self</span><span class="p">.</span><span class="n">b</span>

<span class="c1"># Using the Functional API
</span><span class="k">class</span> <span class="nc">LinearRegressorTF</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">models</span><span class="p">.</span><span class="n">Model</span><span class="p">):</span>
  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">in_dim</span><span class="p">,</span> <span class="n">out_dim</span><span class="p">):</span>
    <span class="nf">super</span><span class="p">(</span><span class="n">LinearRegressorTF</span><span class="p">,</span> <span class="n">self</span><span class="p">).</span><span class="nf">__init__</span><span class="p">()</span>
    <span class="n">self</span><span class="p">.</span><span class="n">linear_layer</span> <span class="o">=</span> <span class="nc">LinearLayerTF</span><span class="p">(</span><span class="n">in_dim</span><span class="p">,</span> <span class="n">out_dim</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="nf">linear_layer</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

<span class="c1"># Instantiate the model
</span><span class="n">linear_functional_model</span> <span class="o">=</span> <span class="nc">LinearRegressorTF</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>

<span class="c1"># Compile the model
</span><span class="n">linear_functional_model</span><span class="p">.</span><span class="nf">compile</span><span class="p">(</span><span class="n">optimizer</span><span class="o">=</span><span class="n">optimizers</span><span class="p">.</span><span class="nc">SGD</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">),</span>
                                <span class="n">loss</span><span class="o">=</span><span class="n">losses</span><span class="p">.</span><span class="nc">MeanSquaredError</span><span class="p">())</span>

<span class="n">linear_functional_model</span><span class="p">.</span><span class="nf">summary</span><span class="p">()</span>

<span class="c1"># Train the model
</span><span class="n">linear_functional_model</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="nf">convert_to_tensor</span><span class="p">(</span><span class="n">X</span><span class="p">),</span>
                            <span class="n">tf</span><span class="p">.</span><span class="nf">convert_to_tensor</span><span class="p">(</span><span class="n">y</span><span class="p">),</span>
                            <span class="n">epochs</span><span class="o">=</span><span class="mi">50</span><span class="p">)</span>

<span class="n">plt</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="sh">'</span><span class="s">.</span><span class="sh">'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">'</span><span class="s">Observed Data</span><span class="sh">'</span><span class="p">)</span>
<span class="n">X_test</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="nf">reshape</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="nf">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">100</span><span class="p">),</span> <span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">linear_functional_model</span><span class="p">.</span><span class="nf">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">),</span> <span class="n">label</span><span class="o">=</span><span class="sh">'</span><span class="s">Fit</span><span class="sh">'</span><span class="p">)</span></code></pre></figure>

<p><img src="/assets/pytorch_vs_tensorflow/tensorflow_fit.png" alt="TF Fit Image" style="width: 500px; height: 350px" /></p>

<h1 id="conclusion">Conclusion</h1>

<p>Both PyTorch and TensorFlow are popular machine learning frameworks.
Whenever a new feature becomes popular on one platform it almost 
always appears on the other. One reason to choose one over the other 
would be due to :</p>

<ul>
  <li>Style of coding.</li>
  <li>PyTorch has seen a lot of traction in the research community as observed 
through mentions in conference submissions,research articles and github code 
<a href="https://paperswithcode.com/trends?ref=mlcontests">1</a>, <a href="https://mlcontests.com/state-of-competitive-machine-learning-2022/">2</a>.</li>
  <li>Industry seems to adopt more tensorflow. <a href="https://www.tensorflow.org/about/case-studies?filter=all#tf-filters">Some case studie from TF website</a></li>
</ul>]]></content><author><name>T-NIKHIL</name><email>nthota2@jhu.edu</email></author><summary type="html"><![CDATA[]]></summary></entry></feed>